---
title: "DS Capstone Milestone Report"
author: "Ryan Jolicoeur"
date: "November 28, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(tidyverse)
library(lubridate)
library(ggplot2)
library(plotly)
library(tidytext)
library(stringi)
library(igraph)
library(ggraph)


train_size <- 0.7
test_size  <- 1 - train_size
```

## Read in the Data

This is the checkin for the milestone report in this section we will work to:

1. Read in the data
2. Convert the data into dataframes
3. We will use the tidytext package
4. In tidytext we will use the unnest_tokens which will
    + Remove Punctuation
    + Change words to lowercase 
5. From there we will join our dataframe with stop words removed
```{r Data-Files}
train_size <- 0.7
test_size  <- 1 - train_size

blog <- readLines("en_US.blogs.txt", encoding = "UTF-8", warn = FALSE, skipNul = TRUE)
news <- readLines("en_US.news.txt", encoding = "UTF-8", warn = FALSE, skipNul = TRUE)
twitter <- readLines("en_US.twitter.txt", encoding = "UTF-8", warn = FALSE, skipNul = TRUE)

data(stop_words)
news_df    <- data_frame(line = 1:length(news), text = news) %>% mutate(Text_Source = "news")
blog_df    <- data_frame(line = 1:length(blog), text = blog) %>% mutate(Text_Source = "blog")
twitter_df <- data_frame(line = 1:length(twitter), text = twitter) %>% mutate(Text_Source = "twitter")
combined_df <- bind_rows(news_df, blog_df, twitter_df)

news_words     <- news_df %>% unnest_tokens(word, text) 
blog_words     <- blog_df %>% 
                    unnest_tokens(word, text) %>%
                     filter(!str_detect(word,"[0-9]"))
twitter_words  <- twitter_df %>% 
                    unnest_tokens(word, text) %>%
                      filter(!str_detect(word,"[0-9]"))
# combined_words <- combined_df %>% unnest_tokens(word, text)

WPL <- sapply(list(blog,news,twitter),
            function(x) summary(stri_count_words(x))[c('Min.','Mean','Max.')])
colnames(WPL) <- c("News_WPL", "Blog_WPL", "Twitter_WPL")
Word_Counts <- as.data.frame(cbind(nrow(news_words), nrow(blog_words), nrow(twitter_words)))
Line_Counts <- as.data.frame(cbind(length(news), length(blog), length(twitter)))

WPL

Summary_Statistics <- bind_rows(Word_Counts, Line_Counts)
colnames(Summary_Statistics) <- c("News_Statistics", "Blog_Statistics", "Twitter_Statistics")
rownames(Summary_Statistics) <- c("Word_Counts", "Line_Counts")

Summary_Statistics

news_words <- news_words %>%
              anti_join(stop_words, by = c("word"))
blog_words <- blog_words %>%
                anti_join(stop_words, by=c("word"))
twitter_words <- twitter_words %>%
                  anti_join(stop_words, by=c("word"))

combined_words <- bind_rows(news_words, blog_words, twitter_words)
```

## Unigram Plots

Here I am going to do unigram plots for the top 10 words for each category and one plot of the top 20 for all combined terms.  

```{r Unigram plots}
news_words %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "dark green") +
  ggtitle("News Top 10 Words") +
  xlab(NULL) +
  coord_flip()
  
blog_words %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "red") +
  ggtitle("Blog Top 10 Words") +
  xlab(NULL) +
  coord_flip()
  
twitter_words %>%
  count(word, sort = TRUE) %>%
  top_n(10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "#00aced") +
  ggtitle("Twitter Top 10 Words") +
  xlab(NULL) +
  coord_flip()

combined_words %>%
  count(word, sort = TRUE) %>%
  top_n(20) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = "#1dcaff") +
  ggtitle("Combined Top 20 Words") +
  xlab(NULL) +
  coord_flip()

```

## Bigrams

At this juncture we are going do bigram plot to look at the common two word pairings associated with the data.  Due to the size of the dataset I decided to do a sample of the dataset and then construct an igraph to look at the word pairings.  

```{r Bigram plots}
bigram_filtered <- combined_df %>%
                    group_by(Text_Source) %>%
                    sample_frac(0.05) %>%
                    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
                    separate(bigram, c("word1", "word2"), sep = " ") %>%
                    filter(!word1 %in% stop_words$word) %>%
                    filter(!word2 %in% stop_words$word) %>%
                    filter(!str_detect(word1,"[0-9]")) %>%
                    filter(!str_detect(word2,"[0-9]")) 
                    
bigram_united <-    bigram_filtered %>%
                    unite(bigram, word1, word2, sep = " ")  

set.seed(215)
bigram_graph <- bigram_filtered %>% 
                  count(word1, word2, sort = TRUE) %>%
                  filter(n > 35) %>%
                  graph_from_data_frame() 

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

## Trigrams

At this juncture we are going do trigram plot to look at the common three pairings associated with the data.  From there I will plot another igraph to represent the trigrams.  

```{r Trigram plots}
trigram_filtered <- combined_df %>%
                      group_by(Text_Source) %>%
                      sample_frac(0.01) %>%
                      unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
                      separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
                      filter(!word1 %in% stop_words$word) %>%
                      filter(!word2 %in% stop_words$word) %>%
                      filter(!word3 %in% stop_words$word) %>%
                      filter(!str_detect(word1,"[0-9]")) %>%
                      filter(!str_detect(word2,"[0-9]")) %>%
                      filter(!str_detect(word3,"[0-9]"))

trigram_united <-    trigram_filtered %>%
                        unite(bigram, word1, word2, word3, sep = " ")  

set.seed(215)
trigram_graph <- trigram_filtered %>% 
  count(word1, word2, word3, sort = TRUE) %>%
  filter(n > 5) %>%
  graph_from_data_frame() 

ggraph(trigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

## Training and testing
In our next part as we get ready to prepare our data by splitting it into a 70% training set and a 30% testing set that we will explore. In the next part now that EDA has been completed I'll begin the modeling and building the Shiny app.  